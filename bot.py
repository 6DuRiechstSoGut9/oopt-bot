# bot.py
import os
import logging
import json
from pathlib import Path
from typing import List

import numpy as np
import torch
from transformers import AutoTokenizer, AutoModel

from telegram import Update
from telegram.ext import (
    ApplicationBuilder,
    ContextTypes,
    CommandHandler,
    MessageHandler,
    filters,
)

# ---------- CONFIG ----------
TOKEN = os.environ.get("TELEGRAM_BOT_TOKEN")
if not TOKEN:
    raise RuntimeError("TELEGRAM_BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –æ–∫—Ä—É–∂–µ–Ω–∏–∏")

MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"
DOCS_DIR = Path("documents")
BATCH_SIZE = 8
TOP_K = 5

# –§–∞–π–ª—ã –∏–Ω–¥–µ–∫—Å–∞
EMB_PATH = Path("embeddings.dat")
TEXTS_PATH = Path("texts.json")
META_PATH = Path("meta.json")

# ----------------------------
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


def read_txt_file(path: Path) -> str:
    """–ß—Ç–µ–Ω–∏–µ txt —Ñ–∞–π–ª–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∫–∞–º–∏"""
    for encoding in ['utf-8', 'cp1251', 'windows-1251', 'latin-1']:
        try:
            with open(path, "r", encoding=encoding) as f:
                content = f.read().strip()
                if content and len(content) > 10:
                    logger.info(f"‚úÖ –£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω {path.name} –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ {encoding}")
                    return content
        except Exception as e:
            continue
    
    logger.error(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å {path.name} –Ω–∏ –≤ –æ–¥–Ω–æ–π –∫–æ–¥–∏—Ä–æ–≤–∫–µ")
    return ""


def load_documents() -> List[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ TXT –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    DOCS_DIR.mkdir(exist_ok=True)
    texts = []
    
    # –¢–æ–ª—å–∫–æ txt —Ñ–∞–π–ª—ã
    txt_files = sorted(DOCS_DIR.glob("*.txt"))
    
    logger.info(f"üîç –ù–∞–π–¥–µ–Ω–æ TXT —Ñ–∞–π–ª–æ–≤: {len(txt_files)}")
    
    for path in txt_files:
        logger.info(f"üìñ –ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞: {path.name}")
        content = read_txt_file(path)
        
        if content:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã –∏ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –∫–æ—Ä–æ—Ç–∫–∏–µ
            paragraphs = [p.strip() for p in content.split('\n') if len(p.strip()) > 30]
            texts.extend(paragraphs)
            logger.info(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ {len(paragraphs)} –∞–±–∑–∞—Ü–µ–≤ –∏–∑ {path.name}")
        else:
            logger.warning(f"‚ö†Ô∏è –§–∞–π–ª {path.name} –ø—É—Å—Ç–æ–π –∏–ª–∏ –Ω–µ —á–∏—Ç–∞–µ—Ç—Å—è")
    
    if not texts:
        texts = [
            "–î–æ–±–∞–≤—å—Ç–µ .txt —Ñ–∞–π–ª—ã –≤ –ø–∞–ø–∫—É documents –¥–ª—è –ø–æ–∏—Å–∫–∞.",
            "–§–∞–π–ª—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –≤ –∫–æ–¥–∏—Ä–æ–≤–∫–µ UTF-8, CP1251 –∏–ª–∏ Windows-1251.",
            "–ë–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ, –Ω–æ –Ω—É–∂–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞."
        ]
        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–µ–º–æ-—Ç–µ–∫—Å—Ç—ã, —Ç–∞–∫ –∫–∞–∫ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
    
    logger.info(f"üìä –í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(texts)}")
    return texts


class EmbeddingIndex:
    def __init__(self):
        logger.info("ü§ñ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
        self.tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
        self.model = AutoModel.from_pretrained(MODEL_NAME)
        self.texts = []

    def encode_batch(self, texts_batch: List[str]) -> np.ndarray:
        """–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        with torch.no_grad():
            enc = self.tokenizer(
                texts_batch,
                padding=True,
                truncation=True,
                max_length=512,
                return_tensors="pt",
            )
            outputs = self.model(**enc)
            embeddings = self.mean_pooling(outputs, enc['attention_mask'])
            embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
            return embeddings.numpy().astype(np.float32)

    @staticmethod
    def mean_pooling(model_output, attention_mask):
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)
        sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)
        return sum_embeddings / sum_mask

    def build_index(self, texts: List[str]):
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ–∏—Å–∫–æ–≤–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞"""
        logger.info(f"üèóÔ∏è –ù–∞—á–∞–ª–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è {len(texts)} —Ç–µ–∫—Å—Ç–æ–≤...")
        
        self.texts = texts
        n = len(texts)
        dim = self.model.config.hidden_size
        
        # –°–æ–∑–¥–∞–µ–º memmap —Ñ–∞–π–ª –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        embeddings = np.memmap(EMB_PATH, dtype=np.float32, mode='w+', shape=(n, dim))
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á–∞–º–∏ —Å –ø—Ä–æ–≥—Ä–µ—Å—Å–æ–º
        for i in range(0, n, BATCH_SIZE):
            batch = texts[i:i + BATCH_SIZE]
            batch_emb = self.encode_batch(batch)
            embeddings[i:i + len(batch)] = batch_emb
            
            if i % (BATCH_SIZE * 5) == 0:  # –õ–æ–≥–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–µ 5 –±–∞—Ç—á–µ–π
                logger.info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {min(i + BATCH_SIZE, n)}/{n} —Ç–µ–∫—Å—Ç–æ–≤")
        
        embeddings.flush()
        del embeddings
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–µ–∫—Å—Ç—ã –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        with open(TEXTS_PATH, 'w', encoding='utf-8') as f:
            json.dump(texts, f, ensure_ascii=False, indent=2)
        
        meta = {
            "n_texts": n,
            "dim": dim,
            "model": MODEL_NAME
        }
        with open(META_PATH, 'w', encoding='utf-8') as f:
            json.dump(meta, f, indent=2)
        
        logger.info("‚úÖ –ò–Ω–¥–µ–∫—Å —É—Å–ø–µ—à–Ω–æ –ø–æ—Å—Ç—Ä–æ–µ–Ω!")

    def load_index(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏–Ω–¥–µ–∫—Å–∞ —Å –¥–∏—Å–∫–∞"""
        if not TEXTS_PATH.exists():
            raise FileNotFoundError("–§–∞–π–ª texts.json –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        with open(TEXTS_PATH, 'r', encoding='utf-8') as f:
            self.texts = json.load(f)
        
        with open(META_PATH, 'r', encoding='utf-8') as f:
            meta = json.load(f)
        
        n = meta["n_texts"]
        dim = meta["dim"]
        embeddings = np.memmap(EMB_PATH, dtype=np.float32, mode='r', shape=(n, dim))
        
        logger.info(f"üìÇ –ò–Ω–¥–µ–∫—Å –∑–∞–≥—Ä—É–∂–µ–Ω: {n} —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
        return embeddings

    def search(self, query: str, top_k: int = 5):
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ –∏–Ω–¥–µ–∫—Å—É"""
        if not self.texts:
            return []
        
        embeddings = self.load_index()
        query_emb = self.encode_batch([query])[0]
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        scores = embeddings @ query_emb
        
        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ–ø-K —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if top_k >= len(scores):
            top_indices = np.argsort(-scores)
        else:
            top_indices = np.argpartition(-scores, top_k)[:top_k]
            top_indices = top_indices[np.argsort(-scores[top_indices])]
        
        results = []
        for idx in top_indices:
            if scores[idx] > 0.1:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —Å—Ö–æ–¥—Å—Ç–≤–∞
                results.append((int(idx), float(scores[idx])))
        
        return results


class BotApp:
    def __init__(self):
        self.index = None
        self.is_ready = False
        self.init_bot()

    def init_bot(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–æ—Ç–∞ —Å –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ–π –ø–µ—Ä–µ–∏–Ω–¥–µ–∫—Å–∞—Ü–∏–µ–π"""
        try:
            logger.info("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram –±–æ—Ç–∞...")
            
            # –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å
            texts = load_documents()
            self.index = EmbeddingIndex()
            self.index.build_index(texts)
            
            self.is_ready = True
            logger.info("‚úÖ –ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!")
            
        except Exception as e:
            logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            self.is_ready = False

    async def start(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not self.is_ready:
            await update.message.reply_text("üîÑ –ë–æ—ÇËøòÂú®ÂàùÂßãÂåñÔºåËØ∑Á®çÂÄô...")
            return
            
        await update.message.reply_text(
            "ü§ñ *–ë–æ—Ç –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!*\n\n"
            "–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —Ñ—Ä–∞–∑—É –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º.\n"
            "–Ø –Ω–∞–π–¥—É –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã —Ç–µ–∫—Å—Ç–∞.\n\n"
            "_–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π_",
            parse_mode='Markdown'
        )

    async def handle_message(self, update: Update, context: ContextTypes.DEFAULT_TYPE):
        if not self.is_ready:
            await update.message.reply_text("‚è≥ –ë–æ—ÇËøòÂú® –∑–∞–≥—Ä—É–∑–∫–∏ÔºåËØ∑Á®çÁ≠â...")
            return
            
        query = update.message.text.strip()
        if not query:
            await update.message.reply_text("üìù –í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞")
            return
            
        try:
            logger.info(f"üîç –ü–æ–∏—Å–∫ –∑–∞–ø—Ä–æ—Å–∞: {query}")
            results = self.index.search(query, TOP_K)
            
            if not results:
                await update.message.reply_text(
                    "‚ùå –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ.\n"
                    "–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."
                )
                return
                
            response = "üìÑ *–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞:*\n\n"
            for i, (idx, score) in enumerate(results, 1):
                snippet = self.index.texts[idx]
                response += f"*{i}.* –°—Ö–æ–¥—Å—Ç–≤–æ: `{score:.3f}`\n"
                response += f"{snippet}\n\n"
                
            # –û–±—Ä–µ–∑–∞–µ–º –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            if len(response) > 4000:
                response = response[:4000] + "\n\n... (—Å–æ–æ–±—â–µ–Ω–∏–µ –æ–±—Ä–µ–∑–∞–Ω–æ)"
                
            await update.message.reply_text(response, parse_mode='Markdown')
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ: {e}")
            await update.message.reply_text(
                "‚ö†Ô∏è –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–∏—Å–∫–µ. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑ –∏–ª–∏ –ø–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –±–æ—Ç–∞ –∫–æ–º–∞–Ω–¥–æ–π /start"
            )


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞ –±–æ—Ç–∞"""
    logger.info("üéØ –ó–∞–ø—É—Å–∫ Telegram –±–æ—Ç–∞...")
    
    try:
        bot_app = BotApp()
        application = ApplicationBuilder().token(TOKEN).build()
        
        # –†–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏
        application.add_handler(CommandHandler("start", bot_app.start))
        application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, bot_app.handle_message))
        
        logger.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –≤ —Ä–µ–∂–∏–º–µ polling")
        application.run_polling()
        
    except Exception as e:
        logger.critical(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞: {e}")
        raise


if __name__ == "__main__":
    main()
