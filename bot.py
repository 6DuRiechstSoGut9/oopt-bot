import os
import logging
import threading
import telebot
from flask import Flask
from waitress import serve
from dotenv import load_dotenv
import requests
import PyPDF2
import docx
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import chromadb
from chromadb.config import Settings

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.route('/')(lambda: "ü§ñ –ë–æ—Ç –û–û–ü–¢ —Ä–∞–±–æ—Ç–∞–µ—Ç!")

TOKEN = os.getenv('TELEGRAM_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

if not TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
if not DEEPSEEK_API_KEY:
    logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

bot = telebot.TeleBot(TOKEN)

class SemanticDocumentSearch:
    def __init__(self):
        self.documents = []
        self.chunks = []  # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
        self.embeddings = None  # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        self.loaded = False
        self.model = None
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def extract_text_from_file(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        text = ""
        try:
            if file_path.endswith('.pdf'):
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                for paragraph in doc.paragraphs:
                    if paragraph.text:
                        text += paragraph.text + "\n"
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return text
    
    def split_text_into_chunks(self, text, chunk_size=500, overlap=50):
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            if i + chunk_size >= len(words):
                break
                
        return chunks
    
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        documents_dir = 'documents'
        if not os.path.exists(documents_dir):
            logger.warning(f"‚ùå –ü–∞–ø–∫–∞ {documents_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            os.makedirs(documents_dir, exist_ok=True)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {documents_dir}")
            self.create_sample_document()
            return
        
        self.documents = []
        self.chunks = []
        file_count = 0
        chunk_count = 0
        
        for root, dirs, files in os.walk(documents_dir):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt')):
                    file_path = os.path.join(root, file)
                    logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file}")
                    
                    text = self.extract_text_from_file(file_path)
                    if text and len(text.strip()) > 10:
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                        text_chunks = self.split_text_into_chunks(text)
                        
                        for i, chunk in enumerate(text_chunks):
                            self.chunks.append({
                                'file': file,
                                'chunk_id': i,
                                'text': chunk,
                                'full_text': text[:500] + "..."  # –î–ª—è –æ—Ç–ª–∞–¥–∫–∏
                            })
                            chunk_count += 1
                        
                        self.documents.append({
                            'file': file,
                            'text': text,
                            'size': len(text),
                            'chunks': len(text_chunks)
                        })
                        file_count += 1
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if self.chunks and self.model:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {chunk_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
            chunk_texts = [chunk['text'] for chunk in self.chunks]
            self.embeddings = self.model.encode(chunk_texts, show_progress_bar=False)
            logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã")
        
        self.loaded = True
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {file_count}, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {chunk_count}")
        
        if file_count == 0:
            self.create_sample_document()
    
    def create_sample_document(self):
        """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        sample_text = """–û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏

–û—Å–æ–±–æ –æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ –ø—Ä–∏—Ä–æ–¥–Ω—ã–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏ (–û–û–ü–¢) - —ç—Ç–æ —É—á–∞—Å—Ç–∫–∏ –∑–µ–º–ª–∏, –∫–æ—Ç–æ—Ä—ã–µ –∏–º–µ—é—Ç –æ—Å–æ–±–æ–µ –ø—Ä–∏—Ä–æ–¥–æ–æ—Ö—Ä–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ.

–ó–∞–∫–∞–∑–Ω–∏–∫–∏ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏:
1. –í–µ—Ä—Ö–Ω–µ-–ê–Ω–¥–æ–º—Å–∫–∏–π –∑–∞–∫–∞–∑–Ω–∏–∫ - —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω –≤ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ, –ø–ª–æ—â–∞–¥—å 4014 –≥–µ–∫—Ç–∞—Ä–æ–≤
2. –ï–∂–æ–∑–µ—Ä—Å–∫–∏–π –∑–∞–∫–∞–∑–Ω–∏–∫ - –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –≤ –ë–∞–±–∞–µ–≤—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ, –ø–ª–æ—â–∞–¥—å 3013 –≥–µ–∫—Ç–∞—Ä–æ–≤  
3. –®–∏–º–æ–∑–µ—Ä—Å–∫–∏–π –∑–∞–∫–∞–∑–Ω–∏–∫ - —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω –≤ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ, –ø–ª–æ—â–∞–¥—å 8553 –≥–µ–∫—Ç–∞—Ä–∞

–ü–∞–º—è—Ç–Ω–∏–∫–∏ –ø—Ä–∏—Ä–æ–¥—ã:
- –ì–µ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–µ –ø–∞–º—è—Ç–Ω–∏–∫–∏ –≤ –ë–∞–±—É—à–∫–∏–Ω—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ
- –ë–æ—Ç–∞–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞–º—è—Ç–Ω–∏–∫–∏ –≤ –í–µ–ª–∏–∫–æ—É—Å—Ç—é–≥—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ
- –õ–∞–Ω–¥—à–∞—Ñ—Ç–Ω—ã–µ –ø–∞–º—è—Ç–Ω–∏–∫–∏ –≤ –ß–µ—Ä–µ–ø–æ–≤–µ—Ü–∫–æ–º —Ä–∞–π–æ–Ω–µ

–í—Å–µ–≥–æ –≤ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –Ω–∞—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –±–æ–ª–µ–µ 100 –æ—Å–æ–±–æ –æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –ø—Ä–∏—Ä–æ–¥–Ω—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è. –û—Å–Ω–æ–≤–Ω—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏: –∑–∞–∫–∞–∑–Ω–∏–∫–∏, –ø–∞–º—è—Ç–Ω–∏–∫–∏ –ø—Ä–∏—Ä–æ–¥—ã, –æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ –ø—Ä–∏—Ä–æ–¥–Ω—ã–µ –ª–∞–Ω–¥—à–∞—Ñ—Ç—ã."""
        
        sample_path = 'documents/–æ–±—Ä–∞–∑–µ—Ü_–æ–æ–ø—Ç.txt'
        os.makedirs('documents', exist_ok=True)
        with open(sample_path, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.load_documents()
        logger.info("üìù –°–æ–∑–¥–∞–Ω –æ–±—Ä–∞–∑–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    def semantic_search(self, query, top_k=3):
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É, –∞ –Ω–µ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
        if not self.loaded or not self.chunks or self.embeddings is None:
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.model.encode([query])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.3:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    chunk = self.chunks[idx]
                    results.append({
                        'file': chunk['file'],
                        'text': chunk['text'],
                        'snippet': chunk['text'][:200] + "...",
                        'score': float(similarities[idx]),
                        'type': 'semantic'
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_documents(self, query):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        return self.semantic_search(query)
    
    def ask_deepseek(self, query, context):
        """–ó–∞–ø—Ä–æ—Å –∫ DeepSeek API"""
        if not DEEPSEEK_API_KEY:
            return "‚ùå API –∫–ª—é—á DeepSeek –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ DEEPSEEK_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è."
        
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –û–û–ü–¢ (–û—Å–æ–±–æ –û—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –ü—Ä–∏—Ä–æ–¥–Ω—ã—Ö –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è—Ö) –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–í–û–ü–†–û–°: {query}

–û—Ç–≤–µ—Ç—å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏: "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."

–û–¢–í–ï–¢:"""

        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system", 
                        "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏. –û—Ç–≤–µ—á–∞–π –¢–û–ß–ù–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 800,
                "temperature": 0.1  # –°–Ω–∏–∂–∞–µ–º —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –¥–ª—è –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
            }
            
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                error_msg = f"–û—à–∏–±–∫–∞ API ({response.status_code})"
                logger.error(f"DeepSeek API error: {response.text}")
                return f"‚ùå {error_msg}"
                
        except Exception as e:
            logger.error(f"DeepSeek request exception: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞
doc_search = SemanticDocumentSearch()

def initialize_documents():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    doc_search.load_documents()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_search.documents)}, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {len(doc_search.chunks)}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
threading.Thread(target=initialize_documents, daemon=True).start()

@bot.message_handler(commands=['start'])
def send_welcome(message):
    status = "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ" if doc_search.loaded else "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞..."
    doc_count = len(doc_search.documents)
    
    welcome_text = f"""ü§ñ –ë–æ—Ç –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏

üìö **–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –æ–± –û—Å–æ–±–æ –û—Ö—Ä–∞–Ω—è–µ–º—ã—Ö –ü—Ä–∏—Ä–æ–¥–Ω—ã—Ö –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è—Ö
{status} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}

üéØ **–¢–µ–ø–µ—Ä—å –±–æ—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –∑–∞–ø—Ä–æ—Å–æ–≤!**

üí° **–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:**
‚Ä¢ "–ö–∞–∫–∏–µ –∑–∞–∫–∞–∑–Ω–∏–∫–∏ –µ—Å—Ç—å –≤ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ?"
‚Ä¢ "–ü–ª–æ—â–∞–¥—å –®–∏–º–æ–∑–µ—Ä—Å–∫–æ–≥–æ –∑–∞–∫–∞–∑–Ω–∏–∫–∞"  
‚Ä¢ "–°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –û–û–ü–¢ –≤ –æ–±–ª–∞—Å—Ç–∏?"
‚Ä¢ "–ü–∞–º—è—Ç–Ω–∏–∫–∏ –ø—Ä–∏—Ä–æ–¥—ã –ë–∞–±—É—à–∫–∏–Ω—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞"

üîç –ë–æ—Ç –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ DeepSeek AI.

üìä –î–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞: /status
üÜò –ü–æ–º–æ—â—å: /help"""
    
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['status'])
def send_status(message):
    doc_count = len(doc_search.documents)
    chunk_count = len(doc_search.chunks) if doc_search.chunks else 0
    has_embeddings = doc_search.embeddings is not None
    
    status_info = f"""üìä **–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:**

‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {'‚úÖ –î–∞' if doc_search.loaded else 'üîÑ –ù–µ—Ç'}
‚Ä¢ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}
‚Ä¢ –¢–µ–∫—Å—Ç–æ–≤—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {chunk_count}
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if has_embeddings else '‚ùå –ù–µ –≥–æ—Ç–æ–≤'}
‚Ä¢ DeepSeek API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if DEEPSEEK_API_KEY else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}

üí° **–ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ!** –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ–± –û–û–ü–¢."""
    
    bot.reply_to(message, status_info)

@bot.message_handler(commands=['help'])
def send_help(message):
    help_text = """üÜò **–ü–æ–º–æ—â—å:**

**–ö–æ–º–∞–Ω–¥—ã:**
/start - –Ω–∞—á–∞—Ç—å —Ä–∞–±–æ—Ç—É
/status - —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã  
/help - —ç—Ç–∞ —Å–ø—Ä–∞–≤–∫–∞

**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å:**
1. –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
2. –ë–æ—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏

**–ü—Ä–∏–º–µ—Ä—ã:**
"–ö–∞–∫–∏–µ –û–û–ü–¢ –≤ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ?"
"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –®–∏–º–æ–∑–µ—Ä—Å–∫–æ–º –∑–∞–∫–∞–∑–Ω–∏–∫–µ"  
"–°–ø–∏—Å–æ–∫ –ø–∞–º—è—Ç–Ω–∏–∫–æ–≤ –ø—Ä–∏—Ä–æ–¥—ã"
"–°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –æ—Ö—Ä–∞–Ω—è–µ–º—ã—Ö —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–π?"

üìö –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç —Å –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –≤ –ø–∞–ø–∫–µ 'documents'"""
    
    bot.reply_to(message, help_text)

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    bot.send_chat_action(message.chat.id, 'typing')
    
    if not doc_search.loaded:
        bot.reply_to(message, "üîÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
        return
    
    user_query = message.text.strip()
    
    if len(user_query) < 2:
        bot.reply_to(message, "‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å.")
        return
    
    # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö —Å –ø–æ–º–æ—â—å—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
    search_results = doc_search.search_documents(user_query)
    
    if search_results:
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        context = "\n\n".join([
            f"[–†–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å: {result['score']:.2f}]\n–§—Ä–∞–≥–º–µ–Ω—Ç: {result['text']}" 
            for result in search_results
        ])
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        answer = doc_search.ask_deepseek(user_query, context)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
        sources = ", ".join(set(result['file'] for result in search_results))
        full_answer = f"{answer}\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}"
        
    else:
        full_answer = f"""‚ùå –ü–æ –∑–∞–ø—Ä–æ—Å—É "{user_query}" –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

üí° **–ü–æ–ø—Ä–æ–±—É–π—Ç–µ:**
‚Ä¢ –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å
‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –æ–±—â–∏–µ –∑–∞–ø—Ä–æ—Å—ã
‚Ä¢ –£–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ä–∞–π–æ–Ω—ã –∏–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏—è

üìã –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ /help –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏."""
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
    try:
        bot.reply_to(message, full_answer)
    except Exception as e:
        logger.error(f"Error sending message: {e}")
        bot.reply_to(message, "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞.")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –ø–æ–∏—Å–∫–æ–º...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    bot_thread = threading.Thread(target=bot.infinity_polling, daemon=True)
    bot_thread.start()
    
    logger.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä
    serve(app, host='0.0.0.0', port=8000)

if __name__ == '__main__':
    main()
