import os
import logging
import threading
import telebot
from flask import Flask
from waitress import serve
from dotenv import load_dotenv
import requests
import PyPDF2
import docx
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.route('/')(lambda: "ü§ñ –ë–æ—Ç –û–û–ü–¢ —Ä–∞–±–æ—Ç–∞–µ—Ç!")

TOKEN = os.getenv('TELEGRAM_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

if not TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
if not DEEPSEEK_API_KEY:
    logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

bot = telebot.TeleBot(TOKEN)

class ImprovedDocumentSearch:
    def __init__(self):
        self.documents = []
        self.chunks = []
        self.embeddings = None
        self.loaded = False
        self.model = None
        
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞...")
            self.model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
    
    def extract_text_from_file(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        text = ""
        try:
            if file_path.endswith('.pdf'):
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                for paragraph in doc.paragraphs:
                    if paragraph.text:
                        text += paragraph.text + "\n"
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return text
    
    def split_text_into_chunks(self, text, chunk_size=800, overlap=100):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏"""
        documents_dir = 'documents'
        if not os.path.exists(documents_dir):
            logger.warning(f"‚ùå –ü–∞–ø–∫–∞ {documents_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            os.makedirs(documents_dir, exist_ok=True)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {documents_dir}")
            self.create_sample_document()
            return
        
        self.documents = []
        self.chunks = []
        file_count = 0
        chunk_count = 0
        
        for root, dirs, files in os.walk(documents_dir):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt')):
                    file_path = os.path.join(root, file)
                    logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file}")
                    
                    text = self.extract_text_from_file(file_path)
                    if text and len(text.strip()) > 10:
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                        text_chunks = self.split_text_into_chunks(text)
                        
                        for i, chunk in enumerate(text_chunks):
                            self.chunks.append({
                                'file': file,
                                'chunk_id': i,
                                'text': chunk,
                                'file_path': file_path
                            })
                            chunk_count += 1
                        
                        self.documents.append({
                            'file': file,
                            'text': text,
                            'size': len(text),
                            'chunks': len(text_chunks)
                        })
                        file_count += 1
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if self.chunks and self.model:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {chunk_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
            chunk_texts = [chunk['text'] for chunk in self.chunks]
            self.embeddings = self.model.encode(chunk_texts, show_progress_bar=False)
            logger.info("‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã")
        
        self.loaded = True
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {file_count}, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {chunk_count}")
    
    def create_sample_document(self):
        """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        sample_text = """–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å - –æ—Å–æ–±–æ –æ—Ö—Ä–∞–Ω—è–µ–º–∞—è –ø—Ä–∏—Ä–æ–¥–Ω–∞—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è. 
        
–û—Ä–Ω–∏—Ç–æ—Ñ–∞—É–Ω–∞ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–≥–æ –ª–µ—Å–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –≤–∏–¥—ã –ø—Ç–∏—Ü:
- –ü–µ–≤—á–∏–µ –ø—Ç–∏—Ü—ã: –∑—è–±–ª–∏–∫, –ø–µ–Ω–æ—á–∫–∞-—Ç–µ–Ω—å–∫–æ–≤–∫–∞, –∑–∞—Ä—è–Ω–∫–∞, –ø–µ–≤—á–∏–π –¥—Ä–æ–∑–¥
- –¢–µ—Ç–µ—Ä–µ–≤–∏–Ω—ã–µ: –≥–ª—É—Ö–∞—Ä—å, —Ä—è–±—á–∏–∫, —Ç–µ—Ç–µ—Ä–µ–≤
- –•–∏—â–Ω—ã–µ –ø—Ç–∏—Ü—ã: —è—Å—Ç—Ä–µ–±-–ø–µ—Ä–µ–ø–µ–ª—è—Ç–Ω–∏–∫, –∫–∞–Ω—é–∫
- –î—è—Ç–ª—ã: –±–æ–ª—å—à–æ–π –ø–µ—Å—Ç—Ä—ã–π –¥—è—Ç–µ–ª, –±–µ–ª–æ—Å–ø–∏–Ω–Ω—ã–π –¥—è—Ç–µ–ª
- –°–æ–≤—ã: —Å–µ—Ä–∞—è –Ω–µ—è—Å—ã—Ç—å, —É—à–∞—Å—Ç–∞—è —Å–æ–≤–∞

–í –ø–µ—Ä–∏–æ–¥ –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ –ª–µ—Å—É –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Å—Ç–∞–∏ –¥—Ä–æ–∑–¥–æ–≤, –∑—è–±–ª–∏–∫–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–ª–µ—Ç–Ω—ã—Ö –ø—Ç–∏—Ü."""
        
        sample_path = 'documents/–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å.docx'
        os.makedirs('documents', exist_ok=True)
        
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π txt —Ñ–∞–π–ª –≤–º–µ—Å—Ç–æ docx –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
        sample_path = 'documents/–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å.txt'
        with open(sample_path, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.load_documents()
        logger.info("üìù –°–æ–∑–¥–∞–Ω –æ–±—Ä–∞–∑–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    def semantic_search(self, query, top_k=5):
        """–£–ª—É—á—à–µ–Ω–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫"""
        if not self.loaded or not self.chunks or self.embeddings is None:
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.model.encode([query])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.2:  # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –±–æ–ª—å—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞
                    chunk = self.chunks[idx]
                    results.append({
                        'file': chunk['file'],
                        'text': chunk['text'],
                        'score': float(similarities[idx]),
                        'file_path': chunk['file_path']
                    })
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∏–∑ –∫–∞–∂–¥–æ–≥–æ
            file_results = {}
            for result in results:
                filename = result['file']
                if filename not in file_results or result['score'] > file_results[filename]['score']:
                    file_results[filename] = result
            
            return list(file_results.values())[:3]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-3 –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_documents(self, query):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫"""
        return self.semantic_search(query)
    
    def generate_intelligent_answer(self, query, search_results):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not search_results:
            return "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        context_parts = []
        for result in search_results:
            context_parts.append(f"–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞ '{result['file']}': {result['text']}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: "{query}"

–ù–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–µ–¥—É—é—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ—Å—Ç–∞–≤—å –ö–†–ê–¢–ö–ò–ô –∏ –ò–ù–§–û–†–ú–ê–¢–ò–í–ù–´–ô –æ—Ç–≤–µ—Ç. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏.

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
- –ë—É–¥—å –∫—Ä–∞—Ç–æ–∫ –∏ —Ç–æ—á–µ–Ω
- –ü–µ—Ä–µ—á–∏—Å–ª–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
- –ï—Å–ª–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –º–∞–ª–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏

–û–¢–í–ï–¢:"""
        
        if not DEEPSEEK_API_KEY:
            # –ï—Å–ª–∏ API –Ω–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ
            files = ", ".join(set(r['file'] for r in search_results))
            best_text = search_results[0]['text'][:300] + "..." if len(search_results[0]['text']) > 300 else search_results[0]['text']
            return f"–ù–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {files}\n\n–û—Å–Ω–æ–≤–Ω—ã–µ —Å–≤–µ–¥–µ–Ω–∏—è: {best_text}"
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system", 
                        "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —Ç–æ—á–Ω–æ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = ", ".join(set(result['file'] for result in search_results))
                return f"{answer}\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}"
            else:
                return f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞. –°—Ç–∞—Ç—É—Å: {response.status_code}"
                
        except Exception as e:
            logger.error(f"DeepSeek request exception: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            files = ", ".join(set(r['file'] for r in search_results))
            return f"–ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {files}\n\n–î–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–æ—á–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ —É–∫–∞–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞
doc_search = ImprovedDocumentSearch()

def initialize_documents():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
    doc_search.load_documents()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_search.documents)}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
threading.Thread(target=initialize_documents, daemon=True).start()

@bot.message_handler(commands=['start', 'help'])
def send_welcome(message):
    welcome_text = """ü§ñ –ë–æ—Ç –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏

üìö **–£–º–Ω—ã–π –ø–æ–∏—Å–∫** –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –æ–± –û–û–ü–¢

üí° **–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å**, –Ω–∞–ø—Ä–∏–º–µ—Ä:
‚Ä¢ "–ö–∞–∫–∏–µ –ø—Ç–∏—Ü—ã –∂–∏–≤—É—Ç –≤ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–º –ª–µ—Å—É?"
‚Ä¢ "–û–û–ü–¢ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞"  
‚Ä¢ "–ü–ª–æ—â–∞–¥—å –®–∏–º–æ–∑–µ—Ä—Å–∫–æ–≥–æ –∑–∞–∫–∞–∑–Ω–∏–∫–∞"

üéØ –ë–æ—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –∏—â–µ—Ç –ø–æ –≤—Å–µ–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º!

üìä –°—Ç–∞—Ç—É—Å: /status"""
    
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['status'])
def send_status(message):
    doc_count = len(doc_search.documents)
    status_info = f"""üìä **–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:**

‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {doc_count}
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if doc_search.loaded else 'üîÑ –ó–∞–≥—Ä—É–∑–∫–∞'}
‚Ä¢ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!

üí° –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –æ–± –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏"""
    
    bot.reply_to(message, status_info)

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    bot.send_chat_action(message.chat.id, 'typing')
    
    if not doc_search.loaded:
        bot.reply_to(message, "üîÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –µ—â–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è... –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É.")
        return
    
    user_query = message.text.strip()
    
    if len(user_query) < 3:
        bot.reply_to(message, "‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å. –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
        return
    
    # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    search_results = doc_search.search_documents(user_query)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    answer = doc_search.generate_intelligent_answer(user_query, search_results)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
    try:
        bot.reply_to(message, answer)
    except Exception as e:
        logger.error(f"Error sending message: {e}")
        bot.reply_to(message, "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞.")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    bot_thread = threading.Thread(target=bot.infinity_polling, daemon=True)
    bot_thread.start()
    
    logger.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä...")
    serve(app, host='0.0.0.0', port=8000)

if __name__ == '__main__':
    main()
