import os
import logging
import threading
import telebot
from flask import Flask
from waitress import serve
from dotenv import load_dotenv
import requests
import PyPDF2
import docx
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import re

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.route('/')(lambda: "ü§ñ –ë–æ—Ç –û–û–ü–¢ —Ä–∞–±–æ—Ç–∞–µ—Ç!")

TOKEN = os.getenv('TELEGRAM_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

if not TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
if not DEEPSEEK_API_KEY:
    logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

bot = telebot.TeleBot(TOKEN)

class LightweightDocumentSearch:
    def __init__(self):
        self.documents = []
        self.chunks = []
        self.vectorizer = None
        self.tfidf_matrix = None
        self.loaded = False
    
    def extract_text_from_file(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        text = ""
        try:
            if file_path.endswith('.pdf'):
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                for paragraph in doc.paragraphs:
                    if paragraph.text:
                        text += paragraph.text + "\n"
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return text
    
    def preprocess_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∏—Å–∫–∞"""
        # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\s+', ' ', text)
        # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
        text = text.lower()
        return text.strip()
    
    def split_text_into_chunks(self, text, chunk_size=1000, overlap=100):
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        words = text.split()
        chunks = []
        
        for i in range(0, len(words), chunk_size - overlap):
            chunk = ' '.join(words[i:i + chunk_size])
            chunks.append(chunk)
            if i + chunk_size >= len(words):
                break
                
        return chunks
    
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å"""
        documents_dir = 'documents'
        if not os.path.exists(documents_dir):
            logger.warning(f"‚ùå –ü–∞–ø–∫–∞ {documents_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            os.makedirs(documents_dir, exist_ok=True)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {documents_dir}")
            self.create_sample_document()
            return
        
        self.documents = []
        self.chunks = []
        file_count = 0
        chunk_count = 0
        
        for root, dirs, files in os.walk(documents_dir):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt')):
                    file_path = os.path.join(root, file)
                    logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file}")
                    
                    text = self.extract_text_from_file(file_path)
                    if text and len(text.strip()) > 10:
                        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
                        cleaned_text = self.preprocess_text(text)
                        
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                        text_chunks = self.split_text_into_chunks(cleaned_text)
                        
                        for i, chunk in enumerate(text_chunks):
                            self.chunks.append({
                                'file': file,
                                'chunk_id': i,
                                'text': chunk,
                                'original_text': text[i*1000:(i+1)*1000] if i == 0 else ""  # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª –¥–ª—è –æ—Ç–≤–µ—Ç–∞
                            })
                            chunk_count += 1
                        
                        self.documents.append({
                            'file': file,
                            'text': cleaned_text,
                            'size': len(text),
                            'chunks': len(text_chunks)
                        })
                        file_count += 1
        
        # –°–æ–∑–¥–∞–µ–º TF-IDF –º–∞—Ç—Ä–∏—Ü—É –¥–ª—è –ø–æ–∏—Å–∫–∞
        if self.chunks:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º –ø–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å –¥–ª—è {chunk_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
            chunk_texts = [chunk['text'] for chunk in self.chunks]
            
            self.vectorizer = TfidfVectorizer(
                max_features=10000,
                min_df=1,
                max_df=0.8,
                stop_words='english'
            )
            self.tfidf_matrix = self.vectorizer.fit_transform(chunk_texts)
            logger.info("‚úÖ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å —Å–æ–∑–¥–∞–Ω")
        
        self.loaded = True
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {file_count}, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {chunk_count}")
    
    def create_sample_document(self):
        """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        sample_text = """–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å - –æ—Å–æ–±–æ –æ—Ö—Ä–∞–Ω—è–µ–º–∞—è –ø—Ä–∏—Ä–æ–¥–Ω–∞—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è –≤ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏.
        
–û—Ä–Ω–∏—Ç–æ—Ñ–∞—É–Ω–∞ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–≥–æ –ª–µ—Å–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –≤–∏–¥—ã –ø—Ç–∏—Ü:
- –ü–µ–≤—á–∏–µ –ø—Ç–∏—Ü—ã: –∑—è–±–ª–∏–∫, –ø–µ–Ω–æ—á–∫–∞-—Ç–µ–Ω—å–∫–æ–≤–∫–∞, –∑–∞—Ä—è–Ω–∫–∞, –ø–µ–≤—á–∏–π –¥—Ä–æ–∑–¥
- –¢–µ—Ç–µ—Ä–µ–≤–∏–Ω—ã–µ: –≥–ª—É—Ö–∞—Ä—å, —Ä—è–±—á–∏–∫, —Ç–µ—Ç–µ—Ä–µ–≤
- –•–∏—â–Ω—ã–µ –ø—Ç–∏—Ü—ã: —è—Å—Ç—Ä–µ–±-–ø–µ—Ä–µ–ø–µ–ª—è—Ç–Ω–∏–∫, –∫–∞–Ω—é–∫
- –î—è—Ç–ª—ã: –±–æ–ª—å—à–æ–π –ø–µ—Å—Ç—Ä—ã–π –¥—è—Ç–µ–ª, –±–µ–ª–æ—Å–ø–∏–Ω–Ω—ã–π –¥—è—Ç–µ–ª
- –°–æ–≤—ã: —Å–µ—Ä–∞—è –Ω–µ—è—Å—ã—Ç—å, —É—à–∞—Å—Ç–∞—è —Å–æ–≤–∞

–í –ø–µ—Ä–∏–æ–¥ –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ –ª–µ—Å—É –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Å—Ç–∞–∏ –¥—Ä–æ–∑–¥–æ–≤, –∑—è–±–ª–∏–∫–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–ª–µ—Ç–Ω—ã—Ö –ø—Ç–∏—Ü. –õ–µ—Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –±–æ–≥–∞—Ç—ã–º –≤–∏–¥–æ–≤—ã–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø—Ç–∏—Ü –±–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –±–∏–æ—Ç–æ–ø–∞–º."""
        
        sample_path = 'documents/–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å.txt'
        os.makedirs('documents', exist_ok=True)
        with open(sample_path, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.load_documents()
        logger.info("üìù –°–æ–∑–¥–∞–Ω –æ–±—Ä–∞–∑–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    def semantic_search(self, query, top_k=5):
        """–ü–æ–∏—Å–∫ –ø–æ —Å–º—ã—Å–ª—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º TF-IDF"""
        if not self.loaded or not self.chunks or self.tfidf_matrix is None:
            return []
        
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∑–∞–ø—Ä–æ—Å –≤ TF-IDF –≤–µ–∫—Ç–æ—Ä
            query_vec = self.vectorizer.transform([self.preprocess_text(query)])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = cosine_similarity(query_vec, self.tfidf_matrix).flatten()
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.1:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    chunk = self.chunks[idx]
                    results.append({
                        'file': chunk['file'],
                        'text': chunk['text'],
                        'score': float(similarities[idx]),
                        'original_text': chunk.get('original_text', chunk['text'])
                    })
            
            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ñ–∞–π–ª–∞–º –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
            file_results = {}
            for result in results:
                filename = result['file']
                if filename not in file_results or result['score'] > file_results[filename]['score']:
                    file_results[filename] = result
            
            return list(file_results.values())[:3]  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–æ–ø-3 –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_documents(self, query):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫"""
        return self.semantic_search(query)
    
    def generate_intelligent_answer(self, query, search_results):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not search_results:
            return "‚ùå –í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.\n\nüí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –û–û–ü–¢."
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        context_parts = []
        for result in search_results:
            context_parts.append(f"–î–æ–∫—É–º–µ–Ω—Ç: {result['file']}\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {result['text']}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç: "{query}"

–ù–∞ –æ—Å–Ω–æ–≤–µ –°–õ–ï–î–£–Æ–©–ï–ô –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ—Å—Ç–∞–≤—å –ö–†–ê–¢–ö–ò–ô –∏ –ò–ù–§–û–†–ú–ê–¢–ò–í–ù–´–ô –æ—Ç–≤–µ—Ç. 
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –Ω–∏—á–µ–≥–æ!

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –æ—Ç–≤–µ—Ç—É:
- –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ —Ç–æ—á–µ–Ω
- –ü–µ—Ä–µ—á–∏—Å–ª–∏ —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–∞–ª–æ, —Ç–∞–∫ –∏ —Å–∫–∞–∂–∏
- –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
- –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º (3-5 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π)

–û–¢–í–ï–¢:"""
        
        if not DEEPSEEK_API_KEY:
            # –ï—Å–ª–∏ API –Ω–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            files = ", ".join(set(r['file'] for r in search_results))
            best_match = search_results[0]['text']
            return f"üìÑ –ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Å—Ö: {files}\n\nüìù –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {best_match[:400]}..."
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system", 
                        "content": "–¢—ã - —Ç–æ—á–Ω—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.1  # –ù–∏–∑–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ—Å—Ç–∏
            }
            
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = ", ".join(set(result['file'] for result in search_results))
                return f"{answer}\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}"
            else:
                logger.error(f"DeepSeek API error: {response.text}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                files = ", ".join(set(r['file'] for r in search_results))
                best_info = "\n".join([f"‚Ä¢ {r['file']}: {r['text'][:200]}..." for r in search_results[:2]])
                return f"üìÑ –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∞–π–¥–µ–Ω–æ:\n\n{best_info}\n\nüìö –§–∞–π–ª—ã: {files}"
                
        except Exception as e:
            logger.error(f"DeepSeek request exception: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            files = ", ".join(set(r['file'] for r in search_results))
            best_info = search_results[0]['text'][:300] + "..." if len(search_results[0]['text']) > 300 else search_results[0]['text']
            return f"üìÑ –ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {files}\n\nüìù –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {best_info}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞
doc_search = LightweightDocumentSearch()

def initialize_documents():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    doc_search.load_documents()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_search.documents)}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
threading.Thread(target=initialize_documents, daemon=True).start()

@bot.message_handler(commands=['start', 'help'])
def send_welcome(message):
    status = "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ" if doc_search.loaded else "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞..."
    doc_count = len(doc_search.documents)
    
    welcome_text = f"""ü§ñ –ë–æ—Ç –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏

üìö **–£–º–Ω—ã–π –ø–æ–∏—Å–∫** –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º –æ–± –û–û–ü–¢
{status} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}

üí° **–ü—Ä–æ—Å—Ç–æ –∑–∞–¥–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å**, –Ω–∞–ø—Ä–∏–º–µ—Ä:
‚Ä¢ "–ö–∞–∫–∏–µ –ø—Ç–∏—Ü—ã –∂–∏–≤—É—Ç –≤ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–º –ª–µ—Å—É?"
‚Ä¢ "–û–û–ü–¢ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞"  
‚Ä¢ "–ü–ª–æ—â–∞–¥—å –®–∏–º–æ–∑–µ—Ä—Å–∫–æ–≥–æ –∑–∞–∫–∞–∑–Ω–∏–∫–∞"

üéØ –ë–æ—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –≤–æ–ø—Ä–æ—Å–æ–≤!

üìä –°—Ç–∞—Ç—É—Å: /status"""
    
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['status'])
def send_status(message):
    doc_count = len(doc_search.documents)
    chunk_count = len(doc_search.chunks) if doc_search.chunks else 0
    
    status_info = f"""üìä **–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:**

‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {doc_count}
‚Ä¢ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {chunk_count}
‚Ä¢ –ü–æ–∏—Å–∫–æ–≤—ã–π –∏–Ω–¥–µ–∫—Å: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if doc_search.loaded else 'üîÑ –ó–∞–≥—Ä—É–∑–∫–∞'}
‚Ä¢ DeepSeek API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if DEEPSEEK_API_KEY else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}

üí° –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!"""
    
    bot.reply_to(message, status_info)

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    bot.send_chat_action(message.chat.id, 'typing')
    
    if not doc_search.loaded:
        bot.reply_to(message, "üîÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è... –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ 10 —Å–µ–∫—É–Ω–¥.")
        return
    
    user_query = message.text.strip()
    
    if len(user_query) < 3:
        bot.reply_to(message, "‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å. –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
        return
    
    # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    search_results = doc_search.search_documents(user_query)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    answer = doc_search.generate_intelligent_answer(user_query, search_results)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
    try:
        bot.reply_to(message, answer)
    except Exception as e:
        logger.error(f"Error sending message: {e}")
        bot.reply_to(message, "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞.")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–±–ª–µ–≥—á–µ–Ω–Ω–æ–≥–æ –±–æ—Ç–∞...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    bot_thread = threading.Thread(target=bot.infinity_polling, daemon=True)
    bot_thread.start()
    
    logger.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä...")
    serve(app, host='0.0.0.0', port=8000)

if __name__ == '__main__':
    main()
