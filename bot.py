import os
import logging
import threading
import telebot
from flask import Flask
from waitress import serve
from dotenv import load_dotenv
import requests
import PyPDF2
import docx
import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import re

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.route('/')(lambda: "ü§ñ –ë–æ—Ç –û–û–ü–¢ —Ä–∞–±–æ—Ç–∞–µ—Ç!")

TOKEN = os.getenv('TELEGRAM_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

if not TOKEN:
    logger.error("‚ùå TELEGRAM_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
if not DEEPSEEK_API_KEY:
    logger.warning("‚ö†Ô∏è DEEPSEEK_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

bot = telebot.TeleBot(TOKEN)

class ProfessionalDocumentSearch:
    def __init__(self):
        self.documents = []
        self.chunks = []
        self.embeddings = None
        self.model = None
        self.loaded = False
    
    def extract_text_from_file(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        text = ""
        try:
            if file_path.endswith('.pdf'):
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                for paragraph in doc.paragraphs:
                    if paragraph.text:
                        text += paragraph.text + "\n"
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
        return text
    
    def split_text_into_chunks(self, text, chunk_size=500, overlap=50):
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ –ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"""
        sentences = re.split(r'[.!?]+', text)
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk) + len(sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —Å–æ–∑–¥–∞–µ–º –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –∏–Ω–¥–µ–∫—Å"""
        documents_dir = 'documents'
        if not os.path.exists(documents_dir):
            logger.warning(f"‚ùå –ü–∞–ø–∫–∞ {documents_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            os.makedirs(documents_dir, exist_ok=True)
            logger.info(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {documents_dir}")
            self.create_sample_document()
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        try:
            logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...")
            self.model = SentenceTransformer('all-MiniLM-L6-v2')
            logger.info("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return
        
        self.documents = []
        self.chunks = []
        file_count = 0
        chunk_count = 0
        
        for root, dirs, files in os.walk(documents_dir):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt')):
                    file_path = os.path.join(root, file)
                    logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file}")
                    
                    text = self.extract_text_from_file(file_path)
                    if text and len(text.strip()) > 10:
                        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã
                        text_chunks = self.split_text_into_chunks(text)
                        
                        for i, chunk in enumerate(text_chunks):
                            self.chunks.append({
                                'file': file,
                                'chunk_id': i,
                                'text': chunk,
                                'file_path': file_path
                            })
                            chunk_count += 1
                        
                        self.documents.append({
                            'file': file,
                            'text': text,
                            'size': len(text),
                            'chunks': len(text_chunks)
                        })
                        file_count += 1
        
        # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –≤—Å–µ—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        if self.chunks and self.model:
            logger.info(f"üîÑ –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è {chunk_count} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤...")
            chunk_texts = [chunk['text'] for chunk in self.chunks]
            self.embeddings = self.model.encode(chunk_texts)
            logger.info(f"‚úÖ –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ–∑–¥–∞–Ω—ã. –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {self.embeddings.shape[1]}")
        
        self.loaded = True
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {file_count}, —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤: {chunk_count}")
    
    def create_sample_document(self):
        """–°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –µ—Å–ª–∏ –Ω–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
        sample_text = """–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å - –æ—Å–æ–±–æ –æ—Ö—Ä–∞–Ω—è–µ–º–∞—è –ø—Ä–∏—Ä–æ–¥–Ω–∞—è —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è –≤ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏.
        
–û—Ä–Ω–∏—Ç–æ—Ñ–∞—É–Ω–∞ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–≥–æ –ª–µ—Å–∞ –≤–∫–ª—é—á–∞–µ—Ç —Å–ª–µ–¥—É—é—â–∏–µ –≤–∏–¥—ã –ø—Ç–∏—Ü:
- –ü–µ–≤—á–∏–µ –ø—Ç–∏—Ü—ã: –∑—è–±–ª–∏–∫, –ø–µ–Ω–æ—á–∫–∞-—Ç–µ–Ω—å–∫–æ–≤–∫–∞, –∑–∞—Ä—è–Ω–∫–∞, –ø–µ–≤—á–∏–π –¥—Ä–æ–∑–¥
- –¢–µ—Ç–µ—Ä–µ–≤–∏–Ω—ã–µ: –≥–ª—É—Ö–∞—Ä—å, —Ä—è–±—á–∏–∫, —Ç–µ—Ç–µ—Ä–µ–≤
- –•–∏—â–Ω—ã–µ –ø—Ç–∏—Ü—ã: —è—Å—Ç—Ä–µ–±-–ø–µ—Ä–µ–ø–µ–ª—è—Ç–Ω–∏–∫, –∫–∞–Ω—é–∫
- –î—è—Ç–ª—ã: –±–æ–ª—å—à–æ–π –ø–µ—Å—Ç—Ä—ã–π –¥—è—Ç–µ–ª, –±–µ–ª–æ—Å–ø–∏–Ω–Ω—ã–π –¥—è—Ç–µ–ª
- –°–æ–≤—ã: —Å–µ—Ä–∞—è –Ω–µ—è—Å—ã—Ç—å, —É—à–∞—Å—Ç–∞—è —Å–æ–≤–∞

–í –ø–µ—Ä–∏–æ–¥ –º–∏–≥—Ä–∞—Ü–∏–∏ –≤ –ª–µ—Å—É –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—é—Ç—Å—è —Å—Ç–∞–∏ –¥—Ä–æ–∑–¥–æ–≤, –∑—è–±–ª–∏–∫–æ–≤ –∏ –¥—Ä—É–≥–∏—Ö –ø–µ—Ä–µ–ª–µ—Ç–Ω—ã—Ö –ø—Ç–∏—Ü. –õ–µ—Å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏–∑—É–µ—Ç—Å—è –±–æ–≥–∞—Ç—ã–º –≤–∏–¥–æ–≤—ã–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º –ø—Ç–∏—Ü –±–ª–∞–≥–æ–¥–∞—Ä—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–º –±–∏–æ—Ç–æ–ø–∞–º."""
        
        sample_path = 'documents/–í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–∏–π –ª–µ—Å.txt'
        os.makedirs('documents', exist_ok=True)
        with open(sample_path, 'w', encoding='utf-8') as f:
            f.write(sample_text)
        
        # –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã
        self.load_documents()
        logger.info("üìù –°–æ–∑–¥–∞–Ω –æ–±—Ä–∞–∑–µ—Ü –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    def semantic_search(self, query, top_k=3):
        """–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–≥–æ —Å—Ö–æ–¥—Å—Ç–≤–∞"""
        if not self.loaded or not self.chunks or self.embeddings is None:
            return []
        
        try:
            # –°–æ–∑–¥–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
            query_embedding = self.model.encode([query])
            
            # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
            similarities = cosine_similarity(query_embedding, self.embeddings)[0]
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
            top_indices = np.argsort(similarities)[::-1][:top_k]
            
            results = []
            for idx in top_indices:
                if similarities[idx] > 0.3:  # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
                    chunk = self.chunks[idx]
                    results.append({
                        'file': chunk['file'],
                        'text': chunk['text'],
                        'score': float(similarities[idx]),
                        'file_path': chunk['file_path']
                    })
            
            return results
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def search_documents(self, query):
        """–û—Å–Ω–æ–≤–Ω–æ–π –ø–æ–∏—Å–∫"""
        return self.semantic_search(query)
    
    def generate_intelligent_answer(self, query, search_results):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–π–¥–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        if not search_results:
            return "‚ùå –í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.\n\nüí° –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å –∏–ª–∏ —É—Ç–æ—á–Ω–∏—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–µ –û–û–ü–¢."
        
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å—é —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
        context_parts = []
        for result in search_results:
            context_parts.append(f"–î–æ–∫—É–º–µ–Ω—Ç: {result['file']}\n–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {result['text']}")
        
        context = "\n\n".join(context_parts)
        
        prompt = f"""–ù–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –æ–± –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ –æ—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.

–í–û–ü–†–û–°: {query}

–ò–ù–§–û–†–ú–ê–¶–ò–Ø –ò–ó –î–û–ö–£–ú–ï–ù–¢–û–í:
{context}

–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
1. –û—Ç–≤–µ—Ç—å –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
2. –ë—É–¥—å –∫–æ–Ω–∫—Ä–µ—Ç–µ–Ω –∏ —Ç–æ—á–µ–Ω
3. –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ, —É–∫–∞–∂–∏ —ç—Ç–æ
4. –ù–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
5. –û—Ç–≤–µ—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫—Ä–∞—Ç–∫–∏–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º

–û–¢–í–ï–¢:"""
        
        if not DEEPSEEK_API_KEY:
            # –ï—Å–ª–∏ API –Ω–µ—Ç, —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ—Å—Ç–æ–π –æ—Ç–≤–µ—Ç
            files = ", ".join(set(r['file'] for r in search_results))
            best_match = search_results[0]['text']
            return f"üìÑ –ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {files}\n\nüìù –†–µ–ª–µ–≤–∞–Ω—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {best_match}"
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {
                        "role": "system", 
                        "content": "–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏. –û—Ç–≤–µ—á–∞–π —Ç–æ—á–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤. –ù–µ –ø—Ä–∏–¥—É–º—ã–≤–∞–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é."
                    },
                    {
                        "role": "user", 
                        "content": prompt
                    }
                ],
                "max_tokens": 500,
                "temperature": 0.1
            }
            
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                answer = result['choices'][0]['message']['content']
                
                # –î–æ–±–∞–≤–ª—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                sources = ", ".join(set(result['file'] for result in search_results))
                return f"{answer}\n\nüìö –ò—Å—Ç–æ—á–Ω–∏–∫–∏: {sources}"
            else:
                logger.error(f"DeepSeek API error: {response.text}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
                files = ", ".join(set(r['file'] for r in search_results))
                best_info = "\n".join([f"‚Ä¢ {r['file']}: {r['text']}" for r in search_results])
                return f"üìÑ –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∞–π–¥–µ–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n\n{best_info}\n\nüìö –§–∞–π–ª—ã: {files}"
                
        except Exception as e:
            logger.error(f"DeepSeek request exception: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç –±–µ–∑ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏
            files = ", ".join(set(r['file'] for r in search_results))
            best_info = search_results[0]['text']
            return f"üìÑ –ù–∞–π–¥–µ–Ω–æ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö: {files}\n\nüìù –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {best_info}"

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã –ø–æ–∏—Å–∫–∞
doc_search = ProfessionalDocumentSearch()

def initialize_documents():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞...")
    doc_search.load_documents()
    logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(doc_search.documents)}")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é
threading.Thread(target=initialize_documents, daemon=True).start()

@bot.message_handler(commands=['start', 'help'])
def send_welcome(message):
    status = "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ" if doc_search.loaded else "üîÑ –ó–∞–≥—Ä—É–∑–∫–∞..."
    doc_count = len(doc_search.documents)
    
    welcome_text = f"""ü§ñ –ë–æ—Ç –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏

üìö **–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫** –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
{status} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {doc_count}

üí° **–ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–º —è–∑—ã–∫–æ–º:**
‚Ä¢ "–ö–∞–∫–∏–µ –ø—Ç–∏—Ü—ã –∂–∏–≤—É—Ç –≤ –í–µ—Ä—Ö–æ–≤–∏–Ω—Å–∫–æ–º –ª–µ—Å—É?"
‚Ä¢ "–û–û–ü–¢ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–≥–æ —Ä–∞–π–æ–Ω–∞"  
‚Ä¢ "–ü–ª–æ—â–∞–¥—å –®–∏–º–æ–∑–µ—Ä—Å–∫–æ–≥–æ –∑–∞–∫–∞–∑–Ω–∏–∫–∞"

üéØ –ë–æ—Ç –ø–æ–Ω–∏–º–∞–µ—Ç —Å–º—ã—Å–ª –∏ –Ω–∞—Ö–æ–¥–∏—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é!

üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã: /status"""
    
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['status'])
def send_status(message):
    doc_count = len(doc_search.documents)
    chunk_count = len(doc_search.chunks) if doc_search.chunks else 0
    has_embeddings = doc_search.embeddings is not None
    
    status_info = f"""üìä **–°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:**

‚Ä¢ –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {doc_count}
‚Ä¢ –§—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∏–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞–Ω–æ: {chunk_count}
‚Ä¢ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫: {'‚úÖ –ê–∫—Ç–∏–≤–µ–Ω' if has_embeddings else '‚ùå –ù–µ –≥–æ—Ç–æ–≤'}
‚Ä¢ DeepSeek API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if DEEPSEEK_API_KEY else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}

üí° –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!"""
    
    bot.reply_to(message, status_info)

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    bot.send_chat_action(message.chat.id, 'typing')
    
    if not doc_search.loaded:
        bot.reply_to(message, "üîÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è... –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–∏–Ω—É—Ç.")
        return
    
    user_query = message.text.strip()
    
    if len(user_query) < 3:
        bot.reply_to(message, "‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∑–∞–ø—Ä–æ—Å. –£—Ç–æ—á–Ω–∏—Ç–µ –≤–æ–ø—Ä–æ—Å.")
        return
    
    # –ò—â–µ–º –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
    search_results = doc_search.search_documents(user_query)
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç
    answer = doc_search.generate_intelligent_answer(user_query, search_results)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç
    try:
        bot.reply_to(message, answer)
    except Exception as e:
        logger.error(f"Error sending message: {e}")
        bot.reply_to(message, "‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –æ—Ç–≤–µ—Ç–∞.")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ –±–æ—Ç–∞...")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–æ—Ç–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
    bot_thread = threading.Thread(target=bot.infinity_polling, daemon=True)
    bot_thread.start()
    
    logger.info("‚úÖ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω. –ó–∞–ø—É—Å–∫–∞–µ–º –≤–µ–±-—Å–µ—Ä–≤–µ—Ä...")
    serve(app, host='0.0.0.0', port=8000)

if __name__ == '__main__':
    main()
