import os
import logging
import threading
import telebot
from flask import Flask
from waitress import serve
from dotenv import load_dotenv
import requests
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import PyPDF2
import docx
from sentence_transformers import SentenceTransformer

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

app = Flask(__name__)
app.route('/')(lambda: "ü§ñ –ë–æ—Ç –û–û–ü–¢ —Å RAG (DeepSeek) —Ä–∞–±–æ—Ç–∞–µ—Ç!")

TOKEN = os.getenv('TELEGRAM_TOKEN')
DEEPSEEK_API_KEY = os.getenv('DEEPSEEK_API_KEY')

bot = telebot.TeleBot(TOKEN)

class RAGSystem:
    def __init__(self):
        self.documents = []
        self.embeddings = []
        self.chunks = []
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
        self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        
    def extract_text_from_file(self, file_path):
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ —Ñ–∞–π–ª–æ–≤"""
        text = ""
        try:
            if file_path.endswith('.pdf'):
                with open(file_path, 'rb') as file:
                    reader = PyPDF2.PdfReader(file)
                    for page in reader.pages:
                        text += page.extract_text() + "\n"
                        
            elif file_path.endswith('.docx'):
                doc = docx.Document(file_path)
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                    
            elif file_path.endswith('.txt'):
                with open(file_path, 'r', encoding='utf-8') as file:
                    text = file.read()
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {file_path}: {e}")
            
        return text
    
    def split_into_chunks(self, text, chunk_size=500):
        """–†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
        sentences = text.split('. ')
        chunks = []
        current_chunk = ""
        
        for sentence in sentences:
            if len(current_chunk + sentence) < chunk_size:
                current_chunk += sentence + ". "
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + ". "
        
        if current_chunk:
            chunks.append(current_chunk.strip())
            
        return chunks
    
    def get_embedding(self, text):
        """–ü–æ–ª—É—á–∞–µ–º embedding —á–µ—Ä–µ–∑ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)"""
        try:
            embedding = self.embedding_model.encode(text)
            return embedding
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è embedding: {e}")
            return None
    
    def load_documents(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã"""
        if not os.path.exists('documents'):
            logger.warning("–ü–∞–ø–∫–∞ documents –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        self.documents = []
        self.chunks = []
        self.embeddings = []
        
        processed_files = 0
        
        for root, dirs, files in os.walk('documents'):
            for file in files:
                if file.endswith(('.pdf', '.docx', '.txt')):
                    file_path = os.path.join(root, file)
                    logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file}")
                    
                    text = self.extract_text_from_file(file_path)
                    if text and len(text.strip()) > 50:  # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                        file_chunks = self.split_into_chunks(text)
                        for chunk in file_chunks:
                            embedding = self.get_embedding(chunk)
                            if embedding is not None:
                                self.chunks.append(chunk)
                                self.embeddings.append(embedding)
                                self.documents.append({
                                    'file': file,
                                    'chunk_preview': chunk[:100] + '...' if len(chunk) > 100 else chunk
                                })
                        processed_files += 1
        
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {processed_files}")
        logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ —á–∞–Ω–∫–æ–≤: {len(self.chunks)}")
        logger.info(f"–†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {len(self.embeddings)}")
    
    def search_similar_chunks(self, query, top_k=3):
        """–ü–æ–∏—Å–∫ –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        if not self.embeddings:
            return []
        
        query_embedding = self.get_embedding(query)
        if query_embedding is None:
            return []
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = cosine_similarity([query_embedding], self.embeddings)[0]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ø-K –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        top_indices = similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            if similarities[idx] > 0.3:  # –ü–æ–Ω–∏–∂–∞–µ–º –ø–æ—Ä–æ–≥ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–∫—Ä—ã—Ç–∏—è
                results.append({
                    'chunk': self.chunks[idx],
                    'similarity': float(similarities[idx]),
                    'source': f"–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞: {self.documents[idx]['file']}"
                })
        
        return results
    
    def ask_deepseek(self, query, context):
        """–ó–∞–ø—Ä–æ—Å –∫ DeepSeek API"""
        if not DEEPSEEK_API_KEY:
            return "‚ùå API –∫–ª—é—á DeepSeek –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω. –î–æ–±–∞–≤—å—Ç–µ DEEPSEEK_API_KEY –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è."
        
        prompt = f"""–¢—ã - –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –û–û–ü–¢ (–û—Å–æ–±–æ –û—Ö—Ä–∞–Ω—è–µ–º—ã–º –ü—Ä–∏—Ä–æ–¥–Ω—ã–º –¢–µ—Ä—Ä–∏—Ç–æ—Ä–∏—è–º) –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏. 
–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ë—É–¥—å —Ç–æ—á–Ω—ã–º –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º.

–ö–û–ù–¢–ï–ö–°–¢:
{context}

–í–û–ü–†–û–°: {query}

–ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Å–∫–∞–∂–∏: "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ —ç—Ç–æ–º—É –≤–æ–ø—Ä–æ—Å—É."

–û–¢–í–ï–¢:"""
        
        try:
            headers = {
                "Authorization": f"Bearer {DEEPSEEK_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": "deepseek-chat",
                "messages": [
                    {"role": "system", "content": "–¢—ã –ø–æ–º–æ—â–Ω–∏–∫ –ø–æ –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏."},
                    {"role": "user", "content": prompt}
                ],
                "max_tokens": 1000,
                "temperature": 0.3
            }
            
            response = requests.post(
                "https://api.deepseek.com/chat/completions",
                headers=headers,
                json=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                return result['choices'][0]['message']['content']
            else:
                logger.error(f"–û—à–∏–±–∫–∞ DeepSeek API: {response.status_code} - {response.text}")
                return f"‚ùå –û—à–∏–±–∫–∞ API: {response.status_code}"
                
        except Exception as e:
            logger.error(f"–ò—Å–∫–ª—é—á–µ–Ω–∏–µ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ DeepSeek: {e}")
            return f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è: {str(e)}"
    
    def generate_answer(self, query, context_chunks):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        if not context_chunks:
            return "‚ùå –í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç
        context = "\n\n".join([f"{chunk['source']}\n{chunk['chunk']}" for chunk in context_chunks])
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º DeepSeek –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        answer = self.ask_deepseek(query, context)
        
        return answer

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
rag_system = RAGSystem()

def initialize_rag():
    """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    rag_system.load_documents()
    if rag_system.chunks:
        logger.info("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
    else:
        logger.warning("‚ö†Ô∏è RAG —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã")

# –ó–∞–ø—É—Å–∫–∞–µ–º –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
threading.Thread(target=initialize_rag, daemon=True).start()

@bot.message_handler(commands=['start'])
def send_welcome(message):
    welcome_text = f"""ü§ñ –ë–æ—Ç –û–û–ü–¢ –í–æ–ª–æ–≥–æ–¥—Å–∫–æ–π –æ–±–ª–∞—Å—Ç–∏ —Å RAG

üìö –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DeepSeek –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
üí° –ó–∞–¥–∞–≤–∞–π—Ç–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–º —è–∑—ã–∫–µ!

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤:
‚Ä¢ "–ö–∞–∫–∏–µ –û–û–ü–¢ –µ—Å—Ç—å –≤ –í—ã—Ç–µ–≥–æ—Ä—Å–∫–æ–º —Ä–∞–π–æ–Ω–µ?"
‚Ä¢ "–†–∞—Å—Å–∫–∞–∂–∏ –æ –∑–∞–∫–∞–∑–Ω–∏–∫–µ –ú–æ–¥–Ω–æ"  
‚Ä¢ "–ö–∞–∫–∏–µ —Å–∞–º—ã–µ –∫—Ä—É–ø–Ω—ã–µ –æ—Ö—Ä–∞–Ω—è–µ–º—ã–µ —Ç–µ—Ä—Ä–∏—Ç–æ—Ä–∏–∏?"
‚Ä¢ "–°–∫–æ–ª—å–∫–æ –≤—Å–µ–≥–æ –û–û–ü–¢ –≤ –æ–±–ª–∞—Å—Ç–∏?"

–°—Ç–∞—Ç—É—Å: {'‚úÖ –î–æ–∫—É–º–µ–Ω—Ç—ã –∑–∞–≥—Ä—É–∂–µ–Ω—ã' if rag_system.chunks else 'üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...'}
–ß–∞–Ω–∫–æ–≤: {len(rag_system.chunks)}
"""
    bot.reply_to(message, welcome_text)

@bot.message_handler(commands=['status'])
def send_status(message):
    status_text = f"""üìä –°—Ç–∞—Ç—É—Å RAG —Å–∏—Å—Ç–µ–º—ã:

‚Ä¢ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {len(rag_system.chunks)}
‚Ä¢ –†–∞–∑–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã: {len(rag_system.embeddings)}
‚Ä¢ –ú–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: –ª–æ–∫–∞–ª—å–Ω–∞—è (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)
‚Ä¢ DeepSeek API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if DEEPSEEK_API_KEY else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}

–°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ –≤–æ–ø—Ä–æ—Å–∞–º!"""
    bot.reply_to(message, status_text)

@bot.message_handler(commands=['debug'])
def send_debug(message):
    """–û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è"""
    if not rag_system.chunks:
        bot.reply_to(message, "‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        return
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–∏–º–µ—Ä—ã —á–∞–Ω–∫–æ–≤
    debug_info = "üìã –ü—Ä–∏–º–µ—Ä—ã –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —á–∞–Ω–∫–æ–≤:\n\n"
    for i, chunk in enumerate(rag_system.chunks[:3]):  # –ü–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
        debug_info += f"{i+1}. {chunk[:100]}...\n\n"
    
    debug_info += f"\n–í—Å–µ–≥–æ —á–∞–Ω–∫–æ–≤: {len(rag_system.chunks)}"
    bot.reply_to(message, debug_info)

@bot.message_handler(func=lambda message: True)
def handle_message(message):
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º, —á—Ç–æ –±–æ—Ç –¥—É–º–∞–µ—Ç
    bot.send_chat_action(message.chat.id, 'typing')
    
    if not rag_system.chunks:
        bot.reply_to(message, "üîÑ –î–æ–∫—É–º–µ–Ω—Ç—ã –µ—â–µ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —á–µ—Ä–µ–∑ –º–∏–Ω—É—Ç—É.")
        return
    
    # –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
    similar_chunks = rag_system.search_similar_chunks(message.text, top_k=3)
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
    if similar_chunks:
        answer = rag_system.generate_answer(message.text, similar_chunks)
    else:
        answer = "‚ùå –í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–∞—à –≤–æ–ø—Ä–æ—Å."
    
    bot.reply_to(message, answer)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –∑–∞–ø—É—Å–∫–∞"""
    logger.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞...")
    threading.Thread(target=bot.infinity_polling, daemon=True).start()
    serve(app, host='0.0.0.0', port=8000)

if __name__ == '__main__':
    main()